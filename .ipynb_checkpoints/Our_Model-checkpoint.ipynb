{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2415445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from ir_measures import read_trec_run\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import ir_measures\n",
    "from ir_measures import *\n",
    "import os\n",
    "from transformers.utils import WEIGHTS_NAME\n",
    "import logging\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "TRAINING_ARGS_NAME = \"training_args.bin\"\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285ef520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(path: str):\n",
    "    pairs = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines(), desc=f'reading pairs from {Path(path).name}'):\n",
    "            qid, did = line.strip().split('\\t')\n",
    "            pairs.append((qid, did))\n",
    "    return pairs\n",
    "\n",
    "def read_triplets(path: str):\n",
    "    triplets = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines(), desc=f'reading triplets from {Path(path).name}'):\n",
    "            qid, pos_id, neg_id = line.strip().split('\\t')\n",
    "            triplets.append((qid, pos_id, neg_id))\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a349fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_path: str,\n",
    "        queries_path: str,\n",
    "        query_doc_pair_path: str,\n",
    "        qrels_path: str = None,\n",
    "        top_k: int = 100,\n",
    "    ):\n",
    "        self.collection = dict(read_pairs(collection_path))\n",
    "        self.queries = dict(read_pairs(queries_path))\n",
    "        with open(qrels_path, 'r') as r:\n",
    "            self.qrels = json.load(r)\n",
    "        self.pairs = []\n",
    "        query_count = {}\n",
    "        for pair in read_trec_run(query_doc_pair_path):\n",
    "            q, d = pair.query_id, pair.doc_id\n",
    "            if q not in query_count:\n",
    "                query_count[q] = 1\n",
    "            elif query_count[q] < top_k:\n",
    "                query_count[q] += 1\n",
    "            self.pairs.append((q, d))\n",
    "        self.top_k = min([max(Counter(pair[0] for pair in self.pairs).values()), top_k])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query_id, doc_id = self.pairs[idx]\n",
    "        query_text = self.queries[query_id]\n",
    "        doc_text = self.collection[doc_id]\n",
    "        return query_id, doc_id, query_text, doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b406c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "\n",
    "    def __init__(self, collection_path: str, queries_path: str, train_triplets_path: str):\n",
    "        self.collection = {}\n",
    "        with open(collection_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                id_, text = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                self.collection[id_] = text\n",
    "\n",
    "        self.queries = {}\n",
    "        with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                id_, text = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                self.queries[id_] = text\n",
    "\n",
    "        self.triplets = read_triplets(train_triplets_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        qid, pid, nid = self.triplets[idx]\n",
    "        q_text = self.queries[qid]\n",
    "        p_text = self.collection[pid]\n",
    "        n_text = self.collection[nid]\n",
    "        return q_text, p_text, n_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a50766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cross attention layer from the transformers library\n",
    "from transformers import BertModel\n",
    "\n",
    "# Modify the MyDenseBiEncoder class to use cross attention\n",
    "class MyDenseBiEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name_or_dir, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        # Use a pre-trained model with a cross attention layer\n",
    "        self.model = BertModel.from_pretrained(model_name_or_dir, add_cross_attention=True)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        # Add a multi head attention layer after the model\n",
    "        self.attention = MultiHeadAttention(n_heads)\n",
    "\n",
    "    def encode(self, input_ids, attention_mask, **kwargs):\n",
    "        # Get the query and document inputs from the kwargs\n",
    "        query_input_ids = kwargs[\"query_input_ids\"]\n",
    "        query_attention_mask = kwargs[\"query_attention_mask\"]\n",
    "        # Pass the query and document inputs to the model with cross attention\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, encoder_hidden_states=query_input_ids, encoder_attention_mask=query_attention_mask, **kwargs)\n",
    "        # Get the last hidden state of the model\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        # Mask padding tokens\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "        last_hidden_state = last_hidden_state.masked_fill(mask == 0, 0.0)\n",
    "        # Apply the multi head attention layer to the last hidden state\n",
    "        attention_output = self.attention(last_hidden_state, last_hidden_state, last_hidden_state, mask)\n",
    "        # Avg over hidden states to get document representation\n",
    "        sum_attention_output = torch.sum(attention_output, dim=1)\n",
    "        sum_attention_mask = torch.clamp(torch.sum(attention_mask, dim=1), min=1e-9)\n",
    "        mean_attention_output = sum_attention_output / sum_attention_mask.unsqueeze(-1)\n",
    "        return mean_attention_output\n",
    "\n",
    "    def score_pairs(self, queries, docs):\n",
    "        q_vectors = self.encode(queries.input_ids, queries.attention_mask)\n",
    "        d_vectors = self.encode(docs.input_ids, docs.attention_mask)\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        scores = cos(q_vectors, d_vectors)\n",
    "        return scores\n",
    "\n",
    "    def forward(self, queries, pos_docs, neg_docs):\n",
    "        pos_scores = self.score_pairs(queries, pos_docs)\n",
    "        neg_scores = self.score_pairs(queries, neg_docs)\n",
    "        loss = self.loss(torch.cat((pos_scores, neg_scores), dim=0), torch.cat((torch.ones_like(pos_scores), torch.zeros_like(neg_scores)), dim=0))\n",
    "        return loss, pos_scores, neg_scores\n",
    "\n",
    "    def save_pretrained(self, model_dir, state_dict=None):\n",
    "        self.model.save_pretrained(model_dir, state_dict=state_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name_or_dir, n_heads):\n",
    "        return cls(model_name_or_dir, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4b76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderTripletCollator:\n",
    "    def __init__(self, tokenizer, query_max_length, doc_max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.query_max_length = query_max_length\n",
    "        self.doc_max_length = doc_max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        queries = []\n",
    "        pos_docs = []\n",
    "        neg_docs = []\n",
    "        for query, pos, neg in batch:\n",
    "            queries.append(query)\n",
    "            pos_docs.append(pos)\n",
    "            neg_docs.append(neg)\n",
    "        queries = self.tokenizer(\n",
    "            queries,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.query_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        pos_docs = self.tokenizer(\n",
    "            pos_docs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.doc_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        neg_docs = self.tokenizer(\n",
    "            neg_docs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.doc_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\"queries\": queries, \"pos_docs\": pos_docs, \"neg_docs\": neg_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d96c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderPairCollator:\n",
    "    def __init__(self, tokenizer, query_max_length, doc_max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.query_max_length = query_max_length\n",
    "        self.doc_max_length = doc_max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        queries = []\n",
    "        docs = []\n",
    "        query_ids = []\n",
    "        doc_ids = []\n",
    "        for qid, did, query, doc in batch:\n",
    "            query_ids.append(qid)\n",
    "            doc_ids.append(did)\n",
    "            queries.append(query)\n",
    "            docs.append(doc)\n",
    "        queries = self.tokenizer(\n",
    "            queries,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.query_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        docs = self.tokenizer(\n",
    "            docs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.doc_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"query_ids\": query_ids,\n",
    "            \"doc_ids\": doc_ids,\n",
    "            \"queries\": queries,\n",
    "            \"docs\": docs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c5bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = \"distilbert-base-uncased\"\n",
    "output_dir = \"output\"\n",
    "epochs = 1\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 16\n",
    "warmup_steps = 5000\n",
    "max_steps = 4000\n",
    "eval_steps = 100\n",
    "lr = 5e-5\n",
    "query_max_length = 100\n",
    "doc_max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da880b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(output_dir) / \"MyDenseBiEncoder\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8ff36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae45efcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 5720: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TripletDataset(\n\u001b[0;32m      2\u001b[0m     collection_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/collection.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     queries_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train_queries.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     train_triplets_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train_triplets.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m dev_dataset \u001b[38;5;241m=\u001b[39m PairDataset(\n\u001b[0;32m      7\u001b[0m     collection_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/collection.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     queries_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/dev_queries.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     query_doc_pair_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/dev_bm25.trec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     qrels_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/dev_qrels.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m )\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mTripletDataset.__init__\u001b[1;34m(self, collection_path, queries_path, train_triplets_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(collection_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m         id_, text \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection[id_] \u001b[38;5;241m=\u001b[39m text\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 5720: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "train_dataset = TripletDataset(\n",
    "    collection_path=\"data/collection.tsv\",\n",
    "    queries_path=\"data/train_queries.tsv\",\n",
    "    train_triplets_path=\"data/train_triplets.tsv\",\n",
    ")\n",
    "dev_dataset = PairDataset(\n",
    "    collection_path=\"data/collection.tsv\",\n",
    "    queries_path=\"data/dev_queries.tsv\",\n",
    "    query_doc_pair_path=\"data/dev_bm25.trec\",\n",
    "    qrels_path=\"data/dev_qrels.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_collator = BiEncoderTripletCollator(tokenizer, query_max_length, doc_max_length)\n",
    "pair_collator = BiEncoderPairCollator(tokenizer, query_max_length, doc_max_length)\n",
    "model = MyDenseBiEncoder(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    warmup_steps=warmup_steps,\n",
    "    metric_for_best_model=\"RR@10\",\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    max_steps=max_steps,\n",
    "    save_steps=eval_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc19627",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HFTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=triplet_collator,\n",
    "    args=training_args,\n",
    "    eval_dataset=dev_dataset,\n",
    "    eval_collator=pair_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6d141de",
   "metadata": {},
   "source": [
    "from cyclical_lr import CyclicalLR\n",
    "\n",
    "my_optimizer = CyclicalLR (model.parameters (), base_lr=0.001, max_lr=0.01, step_size=2000, mode='triangular')\n",
    "\n",
    "trainer = HFTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=triplet_collator,\n",
    "    args=training_args,\n",
    "    eval_dataset=dev_dataset,\n",
    "    eval_collator=pair_collator,\n",
    "    optimizers=(my_optimizer, None),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ab9cdeb",
   "metadata": {},
   "source": [
    "# Import the data collator class from the transformers library\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for cross attention\n",
    "data_collator = DataCollatorWithPadding (\n",
    "    tokenizer=tokenizer, # The tokenizer used for encoding the data\n",
    "    padding=True, # Select a strategy to pad the returned sequences\n",
    "    return_tensors=\"pt\", # The type of Tensor to return\n",
    ")\n",
    "\n",
    "# Import the trainer class from the transformers library\n",
    "from transformers import Trainer\n",
    "\n",
    "# Create a trainer for cross attention\n",
    "trainer = Trainer (\n",
    "    model=model, # The model to train\n",
    "    args=training_args, # The training arguments\n",
    "    data_collator=data_collator, # The data collator\n",
    "    train_dataset=train_dataset, # The training dataset\n",
    "    eval_dataset=eval_dataset, # The evaluation dataset\n",
    "    # Override the get_train_dataloader method to pass the query inputs to the encode method\n",
    "    def get_train_dataloader (self):\n",
    "        # Get the default train dataloader\n",
    "        train_dataloader = super ().get_train_dataloader ()\n",
    "        # Iterate over the batches\n",
    "        for batch in train_dataloader:\n",
    "            # Get the query and document inputs from the batch\n",
    "            query_input_ids = batch [\"query_input_ids\"]\n",
    "            query_attention_mask = batch [\"query_attention_mask\"]\n",
    "            input_ids = batch [\"input_ids\"]\n",
    "            attention_mask = batch [\"attention_mask\"]\n",
    "            # Yield the modified batch with the query inputs\n",
    "            yield {\n",
    "                \"query_input_ids\": query_input_ids,\n",
    "                \"query_attention_mask\": query_attention_mask,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "    # Override the get_eval_dataloader method to pass the query inputs to the encode method\n",
    "    def get_eval_dataloader (self):\n",
    "        # Get the default eval dataloader\n",
    "        eval_dataloader = super ().get_eval_dataloader ()\n",
    "        # Iterate over the batches\n",
    "        for batch in eval_dataloader:\n",
    "            # Get the query and document inputs from the batch\n",
    "            query_input_ids = batch [\"query_input_ids\"]\n",
    "            query_attention_mask = batch [\"query_attention_mask\"]\n",
    "            input_ids = batch [\"input_ids\"]\n",
    "            attention_mask = batch [\"attention_mask\"]\n",
    "            # Yield the modified batch with the query inputs\n",
    "            yield {\n",
    "                \"query_input_ids\": query_input_ids,\n",
    "                \"query_attention_mask\": query_attention_mask,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cf020",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da73677",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PairDataset(\n",
    "    collection_path=\"data/collection.tsv\",\n",
    "    queries_path=\"data/test_queries.tsv\",\n",
    "    query_doc_pair_path=\"data/test_bm25.trec\",\n",
    "    qrels_path=\"data/test_qrels.json\",\n",
    ")\n",
    "\n",
    "test_pair_collator = BiEncoderPairCollator(tokenizer, query_max_length=query_max_length, doc_max_length=doc_max_length)\n",
    "\n",
    "eval_results = trainer.evaluate(test_dataset, test_pair_collator)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
